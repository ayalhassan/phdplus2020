---
title: "Linear Modeling in R"
author: "Clay Ford"
date: "Spring 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE}
# Load some packages we'll use today
library(tidyverse)
library(ggeffects)
library(car)
library(splines)
library(stargazer)
```



## linear modeling with simulated data


### Example 1: one numeric predictor

Let's begin by simulating some fake data

```{r}
x <- 1:25
y <- 10 + 5*x
plot(x, y)

```


10 is the intercept, 5 is the slope. y is completely determined by x

Let's add some "noise" to our data by adding random draws from a Normal
distribution with mean = 0 and a standard deviation = 10.

`set.seed(1)` ensures we all get the same "random" data

```{r}
set.seed(1)
noise <- rnorm(n = 25, mean = 0, sd = 10)

```


Add the noise to 10 + 5*x and re-draw plot

```{r}
y <- 10 + 5*x + noise
plot(x, y)

```


This data is the combination of two parts:

1. 10 + 5*x
2. rnorm(n = 25, mean = 0, sd = 10)

What if we were given this data and told to determine the process that generated it? In other words, fill in the blanks:

1. 
2. rnorm(n = 25, mean = 0, sd = ____)

That's basically what linear modeling is.

Traditional linear modeling assumes the following (among others):

1) the formula is a weighted sum of predictors (eg, 10 + 5*x)
2) the noise is a random draw from a Normal distribution with mean = 0
3) the standard deviation of the Normal distribution is constant

Linear modeling tries to recover the weights in the first assumption (10 and 5) and the standard deviation in the 3rd assumption (10).

Let's attempt to recover the data generating process. For this we use the `lm()`
function. We have to specify the formula for the first assumption. The 2nd and
3rd assumptions are built into `lm()`.

"y ~ x" means we think Part 1 of the model is "y = intercept + slope*x". This
tells `lm()` to take our data and find the best intercept and slope. Notice this
is the correct model!

```{r}
mod <- lm(y ~ x)
summary(mod)

```


The model returns the following estimates:

intercept = 11.135
slope = 5.042
sd = 9.7 (Residual standard error)

We could use those estimates to generate data and see if they look similar to
our original data.

```{r}
y2 <- 11.135 + 5.042*x + rnorm(25, 0, 9.7)
plot(x, y)
points(x, y2, col = "red")

```


We could also add the original and fitted lines

```{r}
plot(x, y)
points(x, y2, col = "red")
abline(a = 10, b = 5)
abline(mod, col = "red")

```


We could also compare distributions using histograms

```{r}
hist(y, main = "observed data")
hist(y2, main = "data simulated from model")

```


Or we could compare using density curves (smooth version of histograms)

```{r}
plot(density(y))
lines(density(y2), col = "red")

```


In real life, we DO NOT KNOW the formula of weighted sums, or even if a
formula of weighted sums is appropriate. We also don't know if the Normality
assumption or constant variance assumption of the noise is plausible.


### Example 2: one categorical and one numeric predictor

Again, let's simulate some data. The function `set.seed(15)` ensures we all generate the same data.

```{r}
set.seed(15)

# random sample of "m" and "f"
gender <- sample(c("m", "f"), size = 200, replace = TRUE)

# random numbers from the range of 1 - 20
score <- runif(n = 200, min = 1, max = 20)

# the noise: random draws from a N(0,5) distribution
noise <- rnorm(n = 200, mean = 0, sd = 5)

```


Now we simulate y. The third predictor is an "interaction". That is, the "effect" of score depends on gender.

```{r}
y <- -1 + -3*(gender=="m") + 2*score + 3*(gender=="m")*score + noise
dat <- data.frame(y, gender, score)

```


Scatter plot of y versus x, colored by gender. Now ggplot2 comes in handy.

```{r}
ggplot(dat, aes(x = score, y = y, color = gender)) +
  geom_point()

```


Notice the distribution of y is not Normal. That's OK. The normality assumption is for the noise/error.

```{r}
ggplot(dat, aes(x = y, y = stat(density))) +
  geom_histogram(bins = 20) +
  geom_density()

```


Let's try to recover the "true" values in the formula and the SD of the noise.

To fit an interaction, use a colon.

```{r}
mod2 <- lm(y ~ gender + score + gender:score, data = dat)
# or more succinctly
mod2 <- lm(y ~ gender * score, data = dat)

```

We come pretty close to recovering the true values...

```{r}
# y <- -1 + -3*(gender=="m") + 2*score + 3*(gender=="m")*score + noise
summary(mod2)

```


Let's simulate data from our model and see how it compares to the original
data. For this we can use the `simulate()` function

```{r}
sim_y <- simulate(mod2)

```

plot both `y` and `sim.y` and compare. This time we use ggplot2.

```{r}
ggplot(dat, aes(x = y)) +
  geom_density() +
  geom_density(aes(sim_1), sim_y, color = "red")

```


Or the base R way

```{r}
plot(density(dat$y))
lines(density(sim_y$sim_1), col = "red")

```


By specifying geom_smooth(method = "lm") in the ggplot2 call we can add the
fitted lines.

```{r}
ggplot(dat, aes(x = score, y = y, color = gender)) +
  geom_point() +
  geom_abline(intercept = -1, slope = 2) +  # true line for "f"
  geom_abline(intercept = -4, slope = 5) +  # true line for "m"
  geom_smooth(method = "lm")                # model predicted lines

```


Probably better to use the model to create the plot. These are usually
referred to as "effect plots". Here we use the `ggpredict()` function from the
ggeffects package.

```{r}
eff <- ggpredict(mod2, terms = c("score", "gender"))
eff
plot(eff, add.data = TRUE)

```



Let's fit a "wrong" model with no interaction.

"y ~ gender + score" means "y = intercept + b1 * gender + b2 * score"

The effect of score is the same for both males and female

```{r}
mod3 <- lm(y ~ gender + score, data = dat)
summary(mod3)

```


Compare simulated data to oberserved data; not good!

```{r}
sim_y <- simulate(mod3)
plot(density(dat$y))
lines(density(sim_y$sim_1), col = "red")

```


Plot the fitted model using `ggpredict`

```{r}
eff <- ggpredict(mod3, terms = c("score", "gender"))
plot(eff, add.data = TRUE)

```


Hence this is linear modeling:

1) propose and fit model(s)
2) determine if the model is good
3) use the model to explain relationships or make predictions

## YOUR TURN #1

```{r}
# submit the following code to simulate some data
set.seed(2)
x1 <- sample(1:5, size = 1000, replace = TRUE, 
             prob = c(0.1,0.2,0.3,0.3,0.1))
x2 <- rnorm(n = 1000, mean = 12, sd = 2)
noise <- rnorm(n = 1000, mean = 0, sd = 4)
y <- 5 + 10*x1 + -4*x2 + noise
df <- data.frame(y, x1, x2)

# Use lm() in attempt to recover the "true" values that were used to generate y.


```


## linear modeling with Albemarle County Real Estate data

