---
title: "Linear Modeling in R"
author: "Clay Ford"
date: "Spring 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE}
# Load some packages we'll use today
library(tidyverse)
library(ggeffects)
library(car)
library(splines)
library(stargazer)
```



## linear modeling with simulated data


### Example 1: one numeric predictor

Let's begin by simulating some fake data

```{r}
x <- 1:25
y <- 10 + 5*x
plot(x, y)

```


10 is the intercept, 5 is the slope. y is completely determined by x

Let's add some "noise" to our data by adding random draws from a Normal
distribution with mean = 0 and a standard deviation = 10.

`set.seed(1)` ensures we all get the same "random" data

```{r}
set.seed(1)
noise <- rnorm(n = 25, mean = 0, sd = 10)

```


Add the noise to 10 + 5*x and re-draw plot

```{r}
y <- 10 + 5*x + noise
plot(x, y)

```


This data is the combination of two parts:

1. 10 + 5*x
2. rnorm(n = 25, mean = 0, sd = 10)

What if we were given this data and told to determine the process that generated it? In other words, fill in the blanks:

1. 
2. rnorm(n = 25, mean = 0, sd = ____)

That's basically what linear modeling is.

Traditional linear modeling assumes the following (among others):

1) the formula is a weighted sum of predictors (eg, 10 + 5*x)
2) the noise is a random draw from a Normal distribution with mean = 0
3) the standard deviation of the Normal distribution is constant

Linear modeling tries to recover the weights in the first assumption (10 and 5) and the standard deviation in the 3rd assumption (10).

Let's attempt to recover the data generating process. For this we use the `lm()`
function. We have to specify the formula for the first assumption. The 2nd and
3rd assumptions are built into `lm()`.

"y ~ x" means we think Part 1 of the model is "y = intercept + slope*x". This
tells `lm()` to take our data and find the best intercept and slope. Notice this
is the correct model!

```{r}
mod <- lm(y ~ x)
summary(mod)

```


The model returns the following estimates:

intercept = 11.135
slope = 5.042
sd = 9.7 (Residual standard error)

We could use those estimates to generate data and see if they look similar to
our original data.

```{r}
y2 <- 11.135 + 5.042*x + rnorm(25, 0, 9.7)
plot(x, y)
points(x, y2, col = "red")

```


We could also add the original and fitted lines

```{r}
plot(x, y)
points(x, y2, col = "red")
abline(a = 10, b = 5)
abline(mod, col = "red")

```


We could also compare distributions using histograms

```{r}
hist(y, main = "observed data")
hist(y2, main = "data simulated from model")

```


Or we could compare using density curves (smooth version of histograms)

```{r}
plot(density(y))
lines(density(y2), col = "red")

```


In real life, we DO NOT KNOW the formula of weighted sums, or even if a
formula of weighted sums is appropriate. We also don't know if the Normality
assumption or constant variance assumption of the noise is plausible.


### Example 2: one categorical and one numeric predictor

Again, let's simulate some data. The function `set.seed(15)` ensures we all generate the same data.

```{r}
set.seed(15)

# random sample of "m" and "f"
gender <- sample(c("m", "f"), size = 200, replace = TRUE)

# random numbers from the range of 1 - 20
score <- runif(n = 200, min = 1, max = 20)

# the noise: random draws from a N(0,5) distribution
noise <- rnorm(n = 200, mean = 0, sd = 5)

```


Now we simulate y. The third predictor is an "interaction". That is, the "effect" of score depends on gender.

```{r}
y <- -1 + -3*(gender=="m") + 2*score + 3*(gender=="m")*score + noise
dat <- data.frame(y, gender, score)

```


Scatter plot of y versus x, colored by gender. Now ggplot2 comes in handy.

```{r}
ggplot(dat, aes(x = score, y = y, color = gender)) +
  geom_point()

```


Notice the distribution of y is not Normal. That's OK. The normality assumption is for the noise/error.

```{r}
ggplot(dat, aes(x = y, y = stat(density))) +
  geom_histogram(bins = 20) +
  geom_density()

```


Let's try to recover the "true" values in the formula and the SD of the noise.

To fit an interaction, use a colon.

```{r}
mod2 <- lm(y ~ gender + score + gender:score, data = dat)
# or more succinctly
mod2 <- lm(y ~ gender * score, data = dat)

```

We come pretty close to recovering the true values...

```{r}
# y <- -1 + -3*(gender=="m") + 2*score + 3*(gender=="m")*score + noise
summary(mod2)

```


Let's simulate data from our model and see how it compares to the original
data. For this we can use the `simulate()` function

```{r}
sim_y <- simulate(mod2)

```

plot both `y` and `sim.y` and compare. This time we use ggplot2.

```{r}
ggplot(dat, aes(x = y)) +
  geom_density() +
  geom_density(aes(sim_1), sim_y, color = "red")

```


Or the base R way

```{r}
plot(density(dat$y))
lines(density(sim_y$sim_1), col = "red")

```


By specifying geom_smooth(method = "lm") in the ggplot2 call we can add the
fitted lines.

```{r}
ggplot(dat, aes(x = score, y = y, color = gender)) +
  geom_point() +
  geom_abline(intercept = -1, slope = 2) +  # true line for "f"
  geom_abline(intercept = -4, slope = 5) +  # true line for "m"
  geom_smooth(method = "lm")                # model predicted lines

```


Probably better to use the model to create the plot. These are usually
referred to as "effect plots". Here we use the `ggpredict()` function from the
ggeffects package.

```{r}
eff <- ggpredict(mod2, terms = c("score", "gender"))
eff
plot(eff, add.data = TRUE)

```



Let's fit a "wrong" model with no interaction.

"y ~ gender + score" means "y = intercept + b1 * gender + b2 * score"

The effect of score is the same for both males and female

```{r}
mod3 <- lm(y ~ gender + score, data = dat)
summary(mod3)

```


Compare simulated data to oberserved data; not good!

```{r}
sim_y <- simulate(mod3)
plot(density(dat$y))
lines(density(sim_y$sim_1), col = "red")

```


Plot the fitted model using `ggpredict`

```{r}
eff <- ggpredict(mod3, terms = c("score", "gender"))
plot(eff, add.data = TRUE)

```


Hence this is linear modeling:

1) propose and fit model(s)
2) determine if the model is good
3) use the model to explain relationships or make predictions

## YOUR TURN #1

```{r}
# submit the following code to simulate some data
set.seed(2)
x1 <- sample(1:5, size = 1000, replace = TRUE, 
             prob = c(0.1,0.2,0.3,0.3,0.1))
x2 <- rnorm(n = 1000, mean = 12, sd = 2)
noise <- rnorm(n = 1000, mean = 0, sd = 4)
y <- 5 + 10*x1 + -4*x2 + noise
df <- data.frame(y, x1, x2)

# Use lm() in attempt to recover the "true" values that were used to generate y.


```


## linear modeling with Albemarle County Real Estate data

Recall we (David Martin) downloaded and cleaned over 30,000 records from Albemarle County's office of Geographic Data Services. The final clean version is available as an RDS file on GitHub. The following code will download the file and read into R. Notice we have to assign the result of `readRDS` to a name. This creates an object in our workspace called "homes".

```{r}
URL <- "https://github.com/uvastatlab/phdplus2020/raw/master/data/homes_final.rds"
homes <- readRDS(file = url(URL))
glimpse(homes)
```

Notice we have a lot of variability in totalvalue of the home.

```{r}
summary(homes$totalvalue)
hist(homes$totalvalue)
# homes less than $1,000,000
hist(homes$totalvalue[homes$totalvalue < 1e6])

```


Perhaps we could build a linear model to help us understand that variability and estimate a home's totalvalue given various predictors such as finsqft, censustract, bedrooms, lotsize, cooling, etc. Let's try that with finsqft.

Below we propse that totalvalue = Intercept + slope * finsqft + noise, where noise is assumed to be drawn from a Normal distribution with mean 0 and some unknown standard deviation. Therefore we are estimating three quantities:

1. Intercept
2. slope coefficient for finsqft
3. standard deviation of the Normal distribution with mean = 0

```{r}
m1 <- lm(totalvalue ~ finsqft, data = homes)
summary(m1)
coef(m1)
sigma(m1)
```

A naive interpretation:
- Every additional square foot adds about $276 to the total value of a home.

An informal check of the "goodness" of this model is to use it to generate data and then compare that data to the original data. Below we use `simulate` to generate 50 sets of totalvalues from our model. Then we plot the density of the observed totalvalue and overlay it with the densities of the simulations. 

The syntax `sim1[[i]]` extracts the ith column of the sim1 data frame.

```{r}
sim1 <- simulate(m1, nsim = 50)
plot(density(homes$totalvalue))
for(i in 1:50)lines(density(sim1[[i]]), col = "red")

# tidyverse method
ggplot(homes, aes(x = totalvalue)) +
  geom_line(stat = "density") +
  geom_line(aes(x = totalvalue, group = sim), 
               pivot_longer(sim1, everything(), 
                            names_to = "sim", 
                            values_to = "totalvalue"),
            stat = "density",
            color = "red")

```

This doesn't look good.

Recall our assumptions:

1) the formula is a weighted sum of predictors (eg, 10 + 5*x)
2) the noise is a random draw from a Normal distribution with mean = 0
3) the standard deviation of the Normal distribution is constant

Calling `plot` on our model object produces a series of plots to assess those last two assumptions.

```{r}
plot(m1)
```

*How to interpret plots*

1. Residuals vs Fitted: should have a horizontal line with uniform and
   symmertic scatter of points; if not, evidence that SD is not constant

2. Normal Q-Q: points should lie close to diagonal line; if not, evidence that
    noise is not drawn from N(0, SD)

3. Scale-Location: should have a horizontal line with uniform scatter of
    point; (similar to #1 but easier to detect trend in dispersion)

4. Residuals vs Leverage: points outside the contour lines are influential
    observations
    
Plots 1, 2, and 3 are very concerning

How to address concerns?

Non-constant SD can be evidence of a mispecified model or a very skewed response. Notice that our response is quite skewed:

```{r}
hist(homes$totalvalue)
```


We could try transforming totalvalue to a different scale. A common transformation is a log transformation. This certainly looks more symmetric but definitely has some outlying observations.

```{r}
hist(log(homes$totalvalue))

```



Let's try modeling log-transformed totalvalue and checking our diagnostic plots.

```{r}
m2 <- lm(log(totalvalue) ~ finsqft, data = homes)
plot(m2)

```

The plots look better but there is still evidence of non-constant variance. We may need to include other predictors in our model.

But first how to interpret what we have at the moment?
```{r}
coef(m2) %>% round(4)

```


The response is log transformed, so interpretation of coefficients changes. We
first need to exponentiate and then interpret as multiplicative instead of
additive effect.

```{r}
exp(coef(m2)) %>% round(4)

```

Naive interpretation
 - each additional finished square foot increases price by 0.05%

Let's examine the summary output in detail:

```{r}
summary(m2)
```

